# webCrawler

![Digging through websites since 2025](crawly.png)


A lightweight concurrent web crawler written in Go.  
It crawls a website starting from a given URL, collects links and images, and ensures concurrency is managed with goroutines, mutexes, and wait groups.

Features

ğŸŒ Crawl a website: Starts from a base URL and recursively crawls internal links.  
ğŸ”— Track links: Keeps track of how many times each URL has been discovered.  
ğŸ–¼ Extract images: Collects image URLs from each page.  
âš¡ Concurrent crawling: Uses goroutines to crawl multiple pages at once.  
ğŸ”’ Thread-safe storage: Shared state is protected with a sync.Mutex.  
â³ Concurrency control: Limits the number of simultaneous HTTP requests using a buffered channel.  
ğŸ›‘ Configurable max pages: Stops crawling after reaching a given maximum number of pages.  

Usage

go run . <startURL> <maxConcurrency> <maxPages>

Example:

go run . "https://www.wagslane.dev/
" 3 10

Arguments:

startURL â†’ The URL to begin crawling.

maxConcurrency â†’ The maximum number of concurrent requests (for example: 3).

maxPages â†’ The maximum number of pages to crawl (for example: 10).

How it works

A config struct holds:  
pages â†’ a map[string]int that tracks visited URLs and their counts  
baseURL â†’ the original domain, used to avoid crawling external sites  
mu â†’ a mutex for safe concurrent access to pages  
concurrencyControl â†’ a buffered channel to control goroutine concurrency  
wg â†’ a wait group to wait for all goroutines to finish  
maxPages â†’ a limit on how many pages to crawl  

The crawler:  
Normalizes and checks URLs against the base domain.  
Downloads the HTML of each page.  
Extracts links (a[href]) and images (img[src]).  
Adds new URLs to the crawl queue if they havenâ€™t been seen.  
Stops once the maxPages limit is reached.  
Requirements  
Go 1.20+

Internet access to crawl live sites

Notes

To debug, you can set maxConcurrency to 1 to see crawling happen sequentially.  
Crawling large websites can be slow or blocked by rate limiting.  
Only internal links (same domain as startURL) are crawled.
